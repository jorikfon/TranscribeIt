import Foundation
import AVFoundation
import TranscribeItCore

/// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –¥–∏–∫—Ç–æ—Ä–∞–º –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
public struct DialogueTranscription {
    public struct Turn: Identifiable {
        public let id = UUID()  // –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è SwiftUI
        public let speaker: Speaker
        public let text: String
        public let startTime: TimeInterval  // –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ä–µ–ø–ª–∏–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        public let endTime: TimeInterval    // –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–µ–ø–ª–∏–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

        public enum Speaker {
            case left   // –õ–µ–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 1)
            case right  // –ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª (Speaker 2)

            public var displayName: String {
                switch self {
                case .left: return "Speaker 1"
                case .right: return "Speaker 2"
                }
            }

            public var color: String {
                switch self {
                case .left: return "blue"
                case .right: return "orange"
                }
            }
        }

        public var duration: TimeInterval {
            return endTime - startTime
        }

        public init(speaker: Speaker, text: String, startTime: TimeInterval, endTime: TimeInterval) {
            self.speaker = speaker
            self.text = text
            self.startTime = startTime
            self.endTime = endTime
        }
    }

    public let turns: [Turn]
    public let isStereo: Bool
    public let totalDuration: TimeInterval  // –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∏–∞–ª–æ–≥–∞

    public init(turns: [Turn], isStereo: Bool, totalDuration: TimeInterval = 0) {
        self.turns = turns
        self.isStereo = isStereo
        self.totalDuration = totalDuration
    }

    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ø–ª–∏–∫–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è timeline)
    public var sortedByTime: [Turn] {
        return turns.sorted { $0.startTime < $1.startTime }
    }

    /// –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥ –∫–∞–∫ —Ç–µ–∫—Å—Ç —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
    public func formatted() -> String {
        if !isStereo || turns.isEmpty {
            return turns.first?.text ?? ""
        }

        return sortedByTime.map { turn in
            let timestamp = formatTimestamp(turn.startTime)
            return "[\(timestamp)] \(turn.speaker.displayName): \(turn.text)"
        }.joined(separator: "\n\n")
    }

    private func formatTimestamp(_ seconds: TimeInterval) -> String {
        let minutes = Int(seconds) / 60
        let secs = Int(seconds) % 60
        let millis = Int((seconds.truncatingRemainder(dividingBy: 1)) * 1000)
        return String(format: "%02d:%02d.%03d", minutes, secs, millis)
    }

    /// –£–±–∏—Ä–∞–µ—Ç –ø–µ—Ä–∏–æ–¥—ã —Ç–∏—à–∏–Ω—ã (–≥–¥–µ –æ–±–∞ —Å–ø–∏–∫–µ—Ä–∞ –º–æ–ª—á–∞—Ç) –∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    /// –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ —Ç–∏—à–∏–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: 2 —Å–µ–∫—É–Ω–¥—ã
    public func removesilencePeriods(minGap: TimeInterval = 2.0) -> DialogueTranscription {
        guard !turns.isEmpty else { return self }

        // –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–ø–ª–∏–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        let sorted = sortedByTime

        var compressedTurns: [Turn] = []
        var currentTime: TimeInterval = 0

        for (index, turn) in sorted.enumerated() {
            let turnDuration = turn.endTime - turn.startTime

            if index == 0 {
                // –ü–µ—Ä–≤–∞—è —Ä–µ–ø–ª–∏–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0
                compressedTurns.append(Turn(
                    speaker: turn.speaker,
                    text: turn.text,
                    startTime: currentTime,
                    endTime: currentTime + turnDuration
                ))
                currentTime += turnDuration
            } else {
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Ä–µ–ø–ª–∏–∫–æ–π
                let previousTurn = sorted[index - 1]
                let gap = turn.startTime - previousTurn.endTime

                // –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—É–∑—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
                // –ò–Ω–∞—á–µ —ç—Ç–æ —Ç–∏—à–∏–Ω–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —É–±—Ä–∞—Ç—å
                if gap < minGap {
                    currentTime += gap
                } else {
                    // –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–∞—É–∑—É (0.5 —Å–µ–∫) –≤–º–µ—Å—Ç–æ –¥–ª–∏–Ω–Ω–æ–π —Ç–∏—à–∏–Ω—ã
                    currentTime += 0.5
                }

                compressedTurns.append(Turn(
                    speaker: turn.speaker,
                    text: turn.text,
                    startTime: currentTime,
                    endTime: currentTime + turnDuration
                ))
                currentTime += turnDuration
            }
        }

        // –ù–æ–≤–∞—è –æ–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å - —ç—Ç–æ –≤—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–µ–ø–ª–∏–∫–∏
        let newTotalDuration = compressedTurns.last?.endTime ?? 0

        LogManager.app.info("–°–∂–∞—Ç–∏–µ –¥–∏–∞–ª–æ–≥–∞: \(String(format: "%.1f", totalDuration))s -> \(String(format: "%.1f", newTotalDuration))s (\(turns.count) —Ä–µ–ø–ª–∏–∫)")

        return DialogueTranscription(
            turns: compressedTurns,
            isStereo: isStereo,
            totalDuration: newTotalDuration
        )
    }
}

/// –°–µ—Ä–≤–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ audio/video —Ñ–∞–π–ª–æ–≤
/// –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç WhisperKit –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç
public class FileTranscriptionService {

    /// –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public enum TranscriptionMode {
        case vad        // –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Voice Activity Detection (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å SpectralVAD –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ)
        case batch      // –ü–∞–∫–µ—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)
    }

    /// –¢–∏–ø VAD –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è —Ä–µ–∂–∏–º–∞ .vad
    public enum VADAlgorithm {
        case standard(VADParameters)       // –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π VAD
        case adaptive(AdaptiveVAD.Parameters)  // –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π VAD —Å ZCR
        case spectral(SpectralVAD.Parameters)  // –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π VAD (FFT)

        /// –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ
        public static let telephone = VADAlgorithm.spectral(.telephone)

        /// –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –¥–ª—è —à–∏—Ä–æ–∫–æ–ø–æ–ª–æ—Å–Ω–æ–≥–æ –∞—É–¥–∏–æ
        public static let wideband = VADAlgorithm.spectral(.wideband)

        /// –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π
        public static let `default` = VADAlgorithm.spectral(.default)
    }

    private let whisperService: WhisperService
    private var batchService: BatchTranscriptionService?

    /// –¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public var mode: TranscriptionMode = .vad  // VAD —Ä–µ–∂–∏–º —Å SpectralVAD –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ

    /// –ê–ª–≥–æ—Ä–∏—Ç–º VAD (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∂–∏–º–µ .vad)
    public var vadAlgorithm: VADAlgorithm = .telephone  // SpectralVAD - Telephone –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    /// Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø—Ä–æ–≥—Ä–µ—Å—Å –∏ —Ä–µ–ø–ª–∏–∫–∏)
    public var onProgressUpdate: ((String, Double, DialogueTranscription?) -> Void)?

    public init(whisperService: WhisperService) {
        self.whisperService = whisperService
        self.batchService = BatchTranscriptionService(
            whisperService: whisperService,
            parameters: .lowQuality
        )
        // –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ UserSettings
        applyUserSettings()
    }

    /// –ü—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VAD –∏–∑ UserSettings
    public func applyUserSettings() {
        let settings = UserSettings.shared

        // –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        switch settings.fileTranscriptionMode {
        case .vad:
            mode = .vad
        case .batch:
            mode = .batch
        }

        // VAD –∞–ª–≥–æ—Ä–∏—Ç–º
        switch settings.vadAlgorithmType {
        case .spectralTelephone:
            vadAlgorithm = .telephone
        case .spectralWideband:
            vadAlgorithm = .wideband
        case .spectralDefault:
            vadAlgorithm = .default
        case .adaptiveLowQuality:
            vadAlgorithm = .adaptive(AdaptiveVAD.Parameters.lowQuality)
        case .adaptiveAggressive:
            vadAlgorithm = .adaptive(AdaptiveVAD.Parameters.aggressive)
        case .standardLowQuality:
            vadAlgorithm = .standard(VADParameters.lowQuality)
        case .standardHighQuality:
            vadAlgorithm = .standard(VADParameters.highQuality)
        case .batch:
            // –î–ª—è batch —Ä–µ–∂–∏–º–∞ VAD –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
            vadAlgorithm = .default
        }

        LogManager.app.info("FileTranscriptionService: –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ - —Ä–µ–∂–∏–º: \(self.mode == .vad ? "VAD" : "Batch"), –∞–ª–≥–æ—Ä–∏—Ç–º: \(self.vadAlgorithmName)")
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å—Ç–µ—Ä–µ–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    /// - Parameter url: URL —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Returns: –î–∏–∞–ª–æ–≥ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ –¥–∏–∫—Ç–æ—Ä–∞–º (–µ—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ)
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public func transcribeFileWithDialogue(at url: URL) async throws -> DialogueTranscription {
        LogManager.app.begin("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –¥–∏–∫—Ç–æ—Ä–æ–≤: \(url.lastPathComponent)")

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Whisper
        if !whisperService.isReady {
            LogManager.app.error("–ú–æ–¥–µ–ª—å Whisper –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ...")
            // –ñ–¥—ë–º –¥–æ 60 —Å–µ–∫—É–Ω–¥ –ø–æ–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è
            for attempt in 1...60 {
                try await Task.sleep(nanoseconds: 1_000_000_000) // 1 —Å–µ–∫—É–Ω–¥–∞
                if whisperService.isReady {
                    LogManager.app.success("–ú–æ–¥–µ–ª—å Whisper –≥–æ—Ç–æ–≤–∞ (–ø–æ–ø—ã—Ç–∫–∞ \(attempt))")
                    break
                }
                if attempt == 60 {
                    LogManager.app.failure("–¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏", message: "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –∑–∞ 60 —Å–µ–∫—É–Ω–¥")
                    throw WhisperError.modelNotLoaded
                }
            }
        }

        LogManager.app.info("–†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: \(self.mode == .batch ? "BATCH" : "VAD (\(self.vadAlgorithmName))")")

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º batch —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω
        if mode == .batch {
            guard let batchService = batchService else {
                throw NSError(domain: "FileTranscriptionService", code: 1,
                            userInfo: [NSLocalizedDescriptionKey: "BatchTranscriptionService –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"])
            }

            // –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º callback –≤ batchService
            batchService.onProgressUpdate = onProgressUpdate

            return try await batchService.transcribe(url: url)
        }

        // VAD —Ä–µ–∂–∏–º (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥)
        // 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—Ç–µ—Ä–µ–æ –ª–∏ —Ñ–∞–π–ª
        let channelCount = try await getChannelCount(from: url)
        LogManager.app.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: \(channelCount)")

        if channelCount == 2 {
            // –°—Ç–µ—Ä–µ–æ: —Ä–∞–∑–¥–µ–ª—è–µ–º –∫–∞–Ω–∞–ª—ã –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
            return try await transcribeStereoAsDialogue(url: url)
        } else {
            // –ú–æ–Ω–æ: –æ–±—ã—á–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            let audioSamples = try await loadAudio(from: url)
            let totalDuration = TimeInterval(audioSamples.count) / 16000.0
            let text = try await whisperService.transcribe(audioSamples: audioSamples)

            LogManager.app.info("–ú–æ–Ω–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(text.count) —Å–∏–º–≤–æ–ª–æ–≤")

            let dialogue = DialogueTranscription(
                turns: [DialogueTranscription.Turn(
                    speaker: .left,
                    text: text,
                    startTime: 0,
                    endTime: totalDuration
                )],
                isStereo: false,
                totalDuration: totalDuration
            )

            // –í—ã–∑—ã–≤–∞–µ–º callback –¥–ª—è –º–æ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Ç–æ–∂–µ
            onProgressUpdate?(url.lastPathComponent, 1.0, dialogue)

            return dialogue
        }
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ —Ñ–∞–π–ª (–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º)
    /// - Parameter url: URL —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Returns: –¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    public func transcribeFile(at url: URL) async throws -> String {
        LogManager.app.begin("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞: \(url.lastPathComponent)")

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Whisper
        if !whisperService.isReady {
            LogManager.app.error("–ú–æ–¥–µ–ª—å Whisper –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ...")
            // –ñ–¥—ë–º –¥–æ 60 —Å–µ–∫—É–Ω–¥ –ø–æ–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è
            for attempt in 1...60 {
                try await Task.sleep(nanoseconds: 1_000_000_000) // 1 —Å–µ–∫—É–Ω–¥–∞
                if whisperService.isReady {
                    LogManager.app.success("–ú–æ–¥–µ–ª—å Whisper –≥–æ—Ç–æ–≤–∞ (–ø–æ–ø—ã—Ç–∫–∞ \(attempt))")
                    break
                }
                if attempt == 60 {
                    LogManager.app.failure("–¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏", message: "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –∑–∞ 60 —Å–µ–∫—É–Ω–¥")
                    throw WhisperError.modelNotLoaded
                }
            }
        }

        // 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –∏–∑ —Ñ–∞–π–ª–∞
        let audioSamples = try await loadAudio(from: url)

        // 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–∏—à–∏–Ω—É
        if SilenceDetector.shared.isSilence(audioSamples) {
            LogManager.app.info("üîá –§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Ç–∏—à–∏–Ω—É")
            throw FileTranscriptionError.silenceDetected
        }

        // 3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º
        let transcription = try await whisperService.transcribe(audioSamples: audioSamples)

        if transcription.isEmpty {
            throw FileTranscriptionError.emptyTranscription
        }

        LogManager.app.success("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Ñ–∞–π–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(transcription.count) —Å–∏–º–≤–æ–ª–æ–≤")
        return transcription
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ –∏–∑ —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç WhisperKit (16kHz mono Float32)
    /// - Parameter url: URL —Ñ–∞–π–ª–∞
    /// - Returns: –ú–∞—Å—Å–∏–≤ audio samples
    /// - Throws: –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    private func loadAudio(from url: URL) async throws -> [Float] {
        let asset = AVAsset(url: url)

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç audio track
        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            LogManager.app.failure("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç audio track", error: FileTranscriptionError.noAudioTrack)
            throw FileTranscriptionError.noAudioTrack
        }

        // –°–æ–∑–¥–∞–µ–º reader –¥–ª—è —á—Ç–µ–Ω–∏—è –∞—É–¥–∏–æ
        let reader = try AVAssetReader(asset: asset)

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞: 16kHz, mono, Linear PCM Float32
        let outputSettings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 1,
            AVLinearPCMBitDepthKey: 32,
            AVLinearPCMIsFloatKey: true,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsNonInterleaved: false
        ]

        let output = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: outputSettings)
        reader.add(output)

        guard reader.startReading() else {
            LogManager.app.failure("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—á–∞—Ç—å —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞", error: FileTranscriptionError.readError)
            throw FileTranscriptionError.readError
        }

        var audioSamples: [Float] = []

        // –ß–∏—Ç–∞–µ–º –≤—Å–µ sample buffers
        while let sampleBuffer = output.copyNextSampleBuffer() {
            if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
                let length = CMBlockBufferGetDataLength(blockBuffer)
                var data = Data(count: length)

                _ = data.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
                    CMBlockBufferCopyDataBytes(blockBuffer, atOffset: 0, dataLength: length, destination: bytes.baseAddress!)
                }

                // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Data –≤ [Float]
                let floatArray = data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Float] in
                    let floatPtr = ptr.bindMemory(to: Float.self)
                    return Array(floatPtr)
                }

                audioSamples.append(contentsOf: floatArray)
            }
        }

        reader.cancelReading()

        let durationSeconds = Float(audioSamples.count) / 16000.0
        LogManager.app.success("–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: \(audioSamples.count) samples, \(String(format: "%.1f", durationSeconds))s")

        // –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —É–±—Ä–∞–Ω–æ - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã –ª—é–±–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        // –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º —á–µ—Ä–µ–∑ VAD

        return audioSamples
    }

    /// –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—É–¥–∏–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ —Ñ–∞–π–ª–µ
    private func getChannelCount(from url: URL) async throws -> Int {
        let asset = AVAsset(url: url)
        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            throw FileTranscriptionError.noAudioTrack
        }

        let formatDescriptions = try await audioTrack.load(.formatDescriptions)
        guard let formatDescription = formatDescriptions.first else {
            return 1 // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–æ–Ω–æ
        }

        if let audioStreamBasicDescription = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription) {
            return Int(audioStreamBasicDescription.pointee.mChannelsPerFrame)
        }

        return 1
    }

    /// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –∫–∞–Ω–∞–ª—É
    private struct ChannelSegment {
        let segment: SpeechSegment
        let channel: Int  // 0 = left, 1 = right
        let speaker: DialogueTranscription.Turn.Speaker
        let audioSamples: [Float]
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Å—Ç–µ—Ä–µ–æ —Ñ–∞–π–ª –∫–∞–∫ –¥–∏–∞–ª–æ–≥ (–ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã –æ—Ç–¥–µ–ª—å–Ω–æ)
    /// –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–õ–ì–û–†–ò–¢–ú: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –≤ —à–∞—Ö–º–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏,
    /// –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥—ã–¥—É—â–∏–π –¥–∏–∞–ª–æ–≥ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
    private func transcribeStereoAsDialogue(url: URL) async throws -> DialogueTranscription {
        LogManager.app.info("üéß –°—Ç–µ—Ä–µ–æ —Ä–µ–∂–∏–º: —Ä–∞–∑–¥–µ–ª—è–µ–º –∫–∞–Ω–∞–ª—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∏–∫—Ç–æ—Ä–æ–≤")

        // 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ
        let stereoSamples = try await loadAudioStereo(from: url)

        // 2. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã
        let leftChannel = extractChannel(from: stereoSamples, channel: 0)
        let rightChannel = extractChannel(from: stereoSamples, channel: 1)

        // 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        let totalDuration = TimeInterval(leftChannel.count) / 16000.0

        // 4. –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π VAD –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏ –≤ –∫–∞–∂–¥–æ–º –∫–∞–Ω–∞–ª–µ
        LogManager.app.info("üé§ VAD: –∞–Ω–∞–ª–∏–∑ –ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (–∞–ª–≥–æ—Ä–∏—Ç–º: \(self.vadAlgorithmName))...")
        let leftSegments = detectSegments(in: leftChannel)
        LogManager.app.info("–ù–∞–π–¥–µ–Ω–æ \(leftSegments.count) —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏ –≤ –ª–µ–≤–æ–º –∫–∞–Ω–∞–ª–µ")

        LogManager.app.info("üé§ VAD: –∞–Ω–∞–ª–∏–∑ –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (–∞–ª–≥–æ—Ä–∏—Ç–º: \(self.vadAlgorithmName))...")
        let rightSegments = detectSegments(in: rightChannel)
        LogManager.app.info("–ù–∞–π–¥–µ–Ω–æ \(rightSegments.count) —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ä–µ—á–∏ –≤ –ø—Ä–∞–≤–æ–º –∫–∞–Ω–∞–ª–µ")

        // 5. –ù–û–í–û–ï: –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ –æ–±–æ–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –∫–∞–Ω–∞–ª—É
        var allSegments: [ChannelSegment] = []

        // –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        for segment in leftSegments {
            let audio = extractSegmentAudio(segment, from: leftChannel)
            allSegments.append(ChannelSegment(
                segment: segment,
                channel: 0,
                speaker: DialogueTranscription.Turn.Speaker.left,
                audioSamples: audio
            ))
        }

        // –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        for segment in rightSegments {
            let audio = extractSegmentAudio(segment, from: rightChannel)
            allSegments.append(ChannelSegment(
                segment: segment,
                channel: 1,
                speaker: DialogueTranscription.Turn.Speaker.right,
                audioSamples: audio
            ))
        }

        // 6. –ù–û–í–û–ï: –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—à–∞—Ö–º–∞—Ç–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫)
        allSegments.sort(by: { $0.segment.startTime < $1.segment.startTime })
        LogManager.app.info("üîÑ –°–µ–≥–º–µ–Ω—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (\(allSegments.count) –≤—Å–µ–≥–æ)")

        // 7. –ù–û–í–û–ï: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –≤ —à–∞—Ö–º–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        var turns: [DialogueTranscription.Turn] = []
        let totalSegments = allSegments.count
        var processedSegments = 0

        for channelSegment in allSegments {
            let segment = channelSegment.segment
            let speaker = channelSegment.speaker
            let segmentAudio = channelSegment.audioSamples

            if !SilenceDetector.shared.isSilence(segmentAudio) {
                // –ù–û–í–û–ï: –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —Ä–µ–ø–ª–∏–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 5)
                let contextPrompt = buildContextPrompt(from: turns, maxTurns: 5)

                let speakerName = speaker == .left ? "Speaker 1" : "Speaker 2"
                LogManager.app.info("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º \(speakerName): \(String(format: "%.1f", segment.startTime))s - \(String(format: "%.1f", segment.endTime))s (–∫–æ–Ω—Ç–µ–∫—Å—Ç: \(contextPrompt.isEmpty ? "–Ω–µ—Ç" : "\(contextPrompt.count) —Å–∏–º–≤–æ–ª–æ–≤"))")

                // –ù–û–í–û–ï: –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ Whisper
                let text = try await whisperService.transcribe(
                    audioSamples: segmentAudio,
                    contextPrompt: contextPrompt.isEmpty ? nil : contextPrompt
                )

                if !text.isEmpty {
                    turns.append(DialogueTranscription.Turn(
                        speaker: speaker,
                        text: text,
                        startTime: segment.startTime,
                        endTime: segment.endTime
                    ))

                    // –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Ä–µ–ø–ª–∏–∫–∏
                    processedSegments += 1
                    let progress = Double(processedSegments) / Double(totalSegments)
                    let partialDialogue = DialogueTranscription(turns: turns, isStereo: true, totalDuration: totalDuration)
                    LogManager.app.debug("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: \(processedSegments)/\(totalSegments), turns: \(turns.count)")
                    onProgressUpdate?(url.lastPathComponent, progress, partialDialogue)
                } else {
                    LogManager.app.warning("\(speakerName): –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ \(String(format: "%.1f", segment.startTime))s")
                }
            }
        }

        LogManager.app.success("–°—Ç–µ—Ä–µ–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: \(turns.count) —Ä–µ–ø–ª–∏–∫ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ)")

        return DialogueTranscription(turns: turns, isStereo: true, totalDuration: totalDuration)
    }

    /// –ù–û–í–û–ï: –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–ø–ª–∏–∫ –¥–∏–∞–ª–æ–≥–∞
    /// –ü–æ–º–æ–≥–∞–µ—Ç Whisper –ª—É—á—à–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∏–º–µ–Ω–∞, —Ç–µ—Ä–º–∏–Ω—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
    private func buildContextPrompt(from turns: [DialogueTranscription.Turn], maxTurns: Int = 5) -> String {
        // –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ä–µ–ø–ª–∏–∫
        let recentTurns = Array(turns.suffix(maxTurns))

        if recentTurns.isEmpty {
            return ""
        }

        // –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –≤–∏–¥–µ –¥–∏–∞–ª–æ–≥–∞
        let context = recentTurns.map { turn in
            let speakerName = turn.speaker == .left ? "Speaker 1" : "Speaker 2"
            return "\(speakerName): \(turn.text)"
        }.joined(separator: " ")

        // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 200-300 —Å–∏–º–≤–æ–ª–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
        let maxLength = 300
        if context.count > maxLength {
            let endIndex = context.index(context.startIndex, offsetBy: maxLength)
            return String(context[..<endIndex]) + "..."
        }

        return context
    }

    /// –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–µ—Ä–µ–æ –∞—É–¥–∏–æ (—Å–æ—Ö—Ä–∞–Ω—è—è –æ–±–∞ –∫–∞–Ω–∞–ª–∞)
    private func loadAudioStereo(from url: URL) async throws -> [[Float]] {
        let asset = AVAsset(url: url)

        guard let audioTrack = try await asset.loadTracks(withMediaType: .audio).first else {
            throw FileTranscriptionError.noAudioTrack
        }

        let reader = try AVAssetReader(asset: asset)

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞: 16kHz, STEREO (2 channels), Linear PCM Float32
        let outputSettings: [String: Any] = [
            AVFormatIDKey: kAudioFormatLinearPCM,
            AVSampleRateKey: 16000,
            AVNumberOfChannelsKey: 2,  // –°—Ç–µ—Ä–µ–æ!
            AVLinearPCMBitDepthKey: 32,
            AVLinearPCMIsFloatKey: true,
            AVLinearPCMIsBigEndianKey: false,
            AVLinearPCMIsNonInterleaved: false  // Interleaved: L, R, L, R, ...
        ]

        let output = AVAssetReaderTrackOutput(track: audioTrack, outputSettings: outputSettings)
        reader.add(output)

        guard reader.startReading() else {
            throw FileTranscriptionError.readError
        }

        var interleavedSamples: [Float] = []

        while let sampleBuffer = output.copyNextSampleBuffer() {
            if let blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer) {
                let length = CMBlockBufferGetDataLength(blockBuffer)
                var data = Data(count: length)

                _ = data.withUnsafeMutableBytes { (bytes: UnsafeMutableRawBufferPointer) in
                    CMBlockBufferCopyDataBytes(blockBuffer, atOffset: 0, dataLength: length, destination: bytes.baseAddress!)
                }

                let floatArray = data.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Float] in
                    let floatPtr = ptr.bindMemory(to: Float.self)
                    return Array(floatPtr)
                }

                interleavedSamples.append(contentsOf: floatArray)
            }
        }

        reader.cancelReading()

        // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –º–∞—Å—Å–∏–≤ –∏–∑ –¥–≤—É—Ö –∫–∞–Ω–∞–ª–æ–≤ (–ø–æ–∫–∞ interleaved)
        return [interleavedSamples]
    }

    /// –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–¥–∏–Ω –∫–∞–Ω–∞–ª –∏–∑ interleaved —Å—Ç–µ—Ä–µ–æ
    private func extractChannel(from stereoData: [[Float]], channel: Int) -> [Float] {
        guard let interleavedSamples = stereoData.first else { return [] }

        var channelSamples: [Float] = []
        channelSamples.reserveCapacity(interleavedSamples.count / 2)

        // Interleaved format: L, R, L, R, L, R, ...
        // channel 0 = left (indices 0, 2, 4, ...)
        // channel 1 = right (indices 1, 3, 5, ...)
        stride(from: channel, to: interleavedSamples.count, by: 2).forEach { index in
            channelSamples.append(interleavedSamples[index])
        }

        let durationSeconds = Float(channelSamples.count) / 16000.0
        LogManager.app.info("–ö–∞–Ω–∞–ª \(channel): \(channelSamples.count) samples, \(String(format: "%.1f", durationSeconds))s")

        return channelSamples
    }

    // MARK: - VAD Helpers

    /// –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã —Ä–µ—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    private func detectSegments(in samples: [Float]) -> [SpeechSegment] {
        switch vadAlgorithm {
        case .standard(let params):
            let vad = VoiceActivityDetector(parameters: params)
            return vad.detectSpeechSegments(in: samples)

        case .adaptive(let params):
            let vad = AdaptiveVAD(parameters: params)
            return vad.detectSpeechSegments(in: samples)

        case .spectral(let params):
            let vad = SpectralVAD(parameters: params)
            return vad.detectSpeechSegments(in: samples)
        }
    }

    /// –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
    private func extractSegmentAudio(_ segment: SpeechSegment, from samples: [Float]) -> [Float] {
        let startIndex = max(0, segment.startSample)
        let endIndex = min(samples.count, segment.endSample)

        guard startIndex < endIndex && startIndex < samples.count else {
            return []
        }

        return Array(samples[startIndex..<endIndex])
    }

    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ VAD –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    private var vadAlgorithmName: String {
        switch vadAlgorithm {
        case .standard:
            return "Standard VAD"
        case .adaptive:
            return "Adaptive VAD"
        case .spectral(let params):
            if params.speechFreqMin == 300 && params.speechFreqMax == 3400 {
                return "Spectral VAD (Telephone)"
            } else if params.speechFreqMin == 80 && params.speechFreqMax == 8000 {
                return "Spectral VAD (Wideband)"
            } else {
                return "Spectral VAD"
            }
        }
    }

}

/// –û—à–∏–±–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤
enum FileTranscriptionError: LocalizedError {
    case noAudioTrack
    case readError
    case silenceDetected
    case emptyTranscription
    case fileTooLarge

    var errorDescription: String? {
        switch self {
        case .noAudioTrack:
            return "File does not contain an audio track"
        case .readError:
            return "Failed to read audio file"
        case .silenceDetected:
            return "File contains only silence"
        case .emptyTranscription:
            return "Transcription resulted in empty text"
        case .fileTooLarge:
            return "File is too large (max 60 minutes)"
        }
    }
}
