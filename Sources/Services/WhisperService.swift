import Foundation
import WhisperKit
import Metal

/// –°–µ—Ä–≤–∏—Å –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ WhisperKit
/// –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏: tiny, base, small
public class WhisperService {
    private var whisperKit: WhisperKit?
    private var modelSize: String  // –ò–∑–º–µ–Ω–µ–Ω–æ —Å let –Ω–∞ var –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏
    private let vocabularyManager = VocabularyManager.shared
    private let audioNormalizer = AudioNormalizer(parameters: .default)

    // Prompt –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    public var promptText: String? = nil

    // –í–∫–ª—é—á–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∞—É–¥–∏–æ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–µ–Ω–æ)
    public var enableNormalization: Bool = true

    // Performance metrics
    public private(set) var lastTranscriptionTime: TimeInterval = 0
    public private(set) var averageRTF: Double = 0  // Real-Time Factor
    private var transcriptionCount: Int = 0
    private var totalRTF: Double = 0

    /// –†–∞–∑–º–µ—Ä —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
    public var currentModelSize: String {
        return modelSize
    }

    public init(modelSize: String = "small") {
        self.modelSize = modelSize
        LogManager.transcription.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è WhisperService —Å –º–æ–¥–µ–ª—å—é \(modelSize)")
    }

    /// –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç WhisperKit —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
    /// - Parameter newModelSize: –†–∞–∑–º–µ—Ä –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (tiny, base, small, medium, large)
    public func reloadModel(newModelSize: String) async throws {
        guard newModelSize != modelSize else {
            LogManager.transcription.info("–ú–æ–¥–µ–ª—å \(newModelSize) —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            return
        }

        LogManager.transcription.begin("–°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏", details: "\(modelSize) ‚Üí \(newModelSize)")

        // –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å—Ç–∞—Ä—É—é –º–æ–¥–µ–ª—å
        whisperKit = nil

        // –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä
        modelSize = newModelSize

        // –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
        try await loadModel()

        // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        resetPerformanceStats()

        LogManager.transcription.success("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–º–µ–Ω–µ–Ω–∞ –Ω–∞ \(newModelSize)")
    }

    /// –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è Apple Silicon
    public func loadModel() async throws {
        LogManager.transcription.begin("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏", details: modelSize)

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —é–Ω–∏—Ç–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ M1 MAX
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º Neural Engine –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
        let computeOptions = ModelComputeOptions(
            melCompute: .cpuAndGPU,              // Mel —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ - GPU
            audioEncoderCompute: .cpuAndNeuralEngine,  // Audio encoder - Neural Engine
            textDecoderCompute: .cpuAndNeuralEngine,   // Text decoder - Neural Engine
            prefillCompute: .cpuAndNeuralEngine        // Prefill - Neural Engine
        )

        do {
            // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è WhisperKit —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            // –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å Hugging Face
            whisperKit = try await WhisperKit(
                model: modelSize,
                computeOptions: computeOptions,
                verbose: true,
                logLevel: .debug,
                prewarm: true  // –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
            )

            LogManager.transcription.success("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞", details: modelSize)

            // –ü—Ä–æ–≤–µ—Ä–∫–∞ Metal acceleration –∏ Neural Engine
            verifyMetalAcceleration()
        } catch {
            LogManager.transcription.failure("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏", error: error)
            throw WhisperError.modelLoadFailed(error)
        }
    }

    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Metal GPU acceleration –∏ Neural Engine
    private func verifyMetalAcceleration() {
        guard let device = MTLCreateSystemDefaultDevice() else {
            LogManager.transcription.error("Metal GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
            return
        }

        let memoryGB = device.recommendedMaxWorkingSetSize / 1024 / 1024 / 1024
        let isAppleSilicon = device.supportsFamily(.apple7)

        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Neural Engine (ANE)
        let supportsANE = device.supportsFamily(.apple7) // M1 –∏ –Ω–æ–≤–µ–µ

        let maxThreads = device.maxThreadsPerThreadgroup

        LogManager.transcription.info("üöÄ Apple Silicon Acceleration")
        LogManager.transcription.info("  GPU: \(device.name)")
        LogManager.transcription.info("  Unified Memory: \(memoryGB)GB")
        LogManager.transcription.info("  Metal: \(isAppleSilicon ? "‚úÖ" : "‚ùå") Apple Silicon")
        LogManager.transcription.info("  Neural Engine: \(supportsANE ? "‚úÖ Enabled (All components)" : "‚ùå")")
        LogManager.transcription.info("  Compute Units: Mel=GPU, Encoder/Decoder/Prefill=ANE")
        LogManager.transcription.debug("  Max threads: \(maxThreads.width)√ó\(maxThreads.height)√ó\(maxThreads.depth)")

        if device.name.contains("M1") {
            LogManager.transcription.info("  üî• M1 detected - using all performance cores + Neural Engine")
        } else if device.name.contains("M2") || device.name.contains("M3") {
            LogManager.transcription.info("  üî• \(device.name) detected - maximum performance mode")
        }
    }

    /// –ë—ã—Å—Ç—Ä–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–∞–Ω–∫–∞ –¥–ª—è real-time –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
    /// - Parameter audioSamples: –ú–∞—Å—Å–∏–≤ Float32 –∞—É–¥–∏–æ —Å—ç–º–ø–ª–æ–≤ (16kHz mono)
    /// - Returns: –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    public func transcribeChunk(audioSamples: [Float]) async throws -> String {
        guard let whisperKit = whisperKit else {
            throw WhisperError.modelNotLoaded
        }

        // –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
        var processedSamples = audioSamples
        if enableNormalization {
            let stats = audioNormalizer.analyze(audioSamples)
            if stats.isQuiet {
                LogManager.transcription.info("–¢–∏—Ö–æ–µ –∞—É–¥–∏–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (RMS=\(stats.rms)), –ø—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é")
                processedSamples = audioNormalizer.normalize(audioSamples)
            }
        }

        // –î–ª—è real-time –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º Quality Enhancement (—Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ)
        // –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        let settings = UserSettings.shared
        let prefillPrompt = settings.buildFullPrefillPrompt()
        let usePrefill = !prefillPrompt.isEmpty

        let options = DecodingOptions(
            task: .transcribe,        // TRANSCRIBE, –Ω–µ translate!
            language: settings.transcriptionLanguage,  // –Ø–∑—ã–∫ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            temperature: 0.0,         // –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            topK: 1,                  // Greedy decoding –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ (–ù–ï beam search –≤ real-time)
            usePrefillPrompt: usePrefill,   // –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä–µ–π –µ—Å–ª–∏ –µ—Å—Ç—å
            usePrefillCache: usePrefill,    // –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            detectLanguage: false     // –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —è–∑—ã–∫ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        )

        let results = try await whisperKit.transcribe(
            audioArray: processedSamples,
            decodeOptions: options
        )

        guard let firstResult = results.first else {
            return ""
        }

        let transcription = firstResult.text.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines)

        // Apply vocabulary corrections
        let correctedText = vocabularyManager.correctTranscription(transcription)

        return correctedText
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
    /// - Parameters:
    ///   - audioSamples: –ú–∞—Å—Å–∏–≤ Float32 –∞—É–¥–∏–æ —Å—ç–º–ø–ª–æ–≤ (16kHz mono)
    ///   - contextPrompt: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏
    /// - Returns: –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    public func transcribe(audioSamples: [Float], contextPrompt: String? = nil) async throws -> String {
        // –í—Ä–µ–º–µ–Ω–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π prompt
        let originalPrompt = self.promptText

        // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
        if let context = contextPrompt, !context.isEmpty {
            self.promptText = context
            LogManager.transcription.debug("–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç: \"\(context.prefix(100))...\"")
        }

        // –í—ã–∑—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        let result = try await transcribeInternal(audioSamples: audioSamples)

        // –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π prompt
        self.promptText = originalPrompt

        return result
    }

    /// –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥)
    /// - Parameter audioSamples: –ú–∞—Å—Å–∏–≤ Float32 –∞—É–¥–∏–æ —Å—ç–º–ø–ª–æ–≤ (16kHz mono)
    /// - Returns: –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    private func transcribeInternal(audioSamples: [Float]) async throws -> String {
        guard let whisperKit = whisperKit else {
            LogManager.transcription.failure("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è", message: "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            throw WhisperError.modelNotLoaded
        }

        let sampleCount = audioSamples.count
        let audioDuration = Double(sampleCount) / 16000.0  // 16kHz sample rate

        LogManager.transcription.begin("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è", details: "\(sampleCount) samples, \(String(format: "%.2f", audioDuration))s")

        // –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
        var processedSamples = audioSamples
        if enableNormalization {
            let stats = audioNormalizer.analyze(audioSamples)
            LogManager.transcription.debug("–ê—É–¥–∏–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: peak=\(String(format: "%.3f", stats.peak)), rms=\(String(format: "%.3f", stats.rms))")

            if stats.isQuiet {
                LogManager.transcription.info("–¢–∏—Ö–æ–µ –∞—É–¥–∏–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (RMS=\(String(format: "%.3f", stats.rms))), –ø—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é")
                processedSamples = audioNormalizer.normalize(audioSamples)

                let normalizedStats = audioNormalizer.analyze(processedSamples)
                LogManager.transcription.success("–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: RMS \(String(format: "%.3f", stats.rms)) ‚Üí \(String(format: "%.3f", normalizedStats.rms))")
            } else {
                LogManager.transcription.debug("–ì—Ä–æ–º–∫–æ—Å—Ç—å –∞—É–¥–∏–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            }
        }

        let startTime = Date()

        do {
            // –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –°–ú–ï–®–ê–ù–ù–û–ô —Ä–µ—á–∏ (RU+EN)
            // –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∂–∏–º –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
            let settings = UserSettings.shared
            let useQualityMode = settings.useQualityEnhancement

            // –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ—Ñ–∏–ª–ª –ø—Ä–æ–º–ø—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä–µ–π –∏ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            let prefillPrompt = settings.buildFullPrefillPrompt()
            let usePrefill = !prefillPrompt.isEmpty

            let options = DecodingOptions(
                task: .transcribe,      // transcribe (–Ω–µ translate!)
                language: settings.transcriptionLanguage,  // –Ø–∑—ã–∫ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "ru")
                temperature: 0.0,       // –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥
                temperatureIncrementOnFallback: useQualityMode && settings.useTemperatureFallback ? 0.2 : 0.0,
                temperatureFallbackCount: useQualityMode && settings.useTemperatureFallback ? 5 : 0,
                topK: useQualityMode ? 5 : 1,  // Beam search: 5 beams vs greedy (1)
                usePrefillPrompt: usePrefill,   // –ò—Å–ø–æ–ª—å–∑—É–µ–º prefill –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–≤–∞—Ä–∏/–ø—Ä–æ–º–ø—Ç
                usePrefillCache: usePrefill,    // –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ prefill
                detectLanguage: false,          // –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —è–∑—ã–∫ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
                compressionRatioThreshold: useQualityMode ? settings.compressionRatioThreshold : nil,
                logProbThreshold: useQualityMode ? settings.logProbThreshold : nil,
                noSpeechThreshold: useQualityMode ? 0.6 : nil  // –§–∏–ª—å—Ç—Ä —Ç–∏—à–∏–Ω—ã
            )

            // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            LogManager.transcription.info("üåê Language: \(settings.transcriptionLanguage)")
            if !settings.selectedDictionaryIds.isEmpty {
                LogManager.transcription.info("üìö Active dictionaries: \(settings.selectedDictionaryIds.joined(separator: ", "))")
            }
            if !settings.customPrefillPrompt.isEmpty {
                LogManager.transcription.info("‚úèÔ∏è  Custom prefill: \(settings.customPrefillPrompt.prefix(50))...")
            }

            if useQualityMode {
                LogManager.transcription.info("‚ú® Quality Enhancement Mode:")
                LogManager.transcription.info("  - Beam search: \(options.topK) beams")
                if settings.useTemperatureFallback {
                    LogManager.transcription.info("  - Temperature fallback: 0.0 ‚Üí 1.0 (5 steps)")
                }
                LogManager.transcription.info("  - Compression ratio filter: \(settings.compressionRatioThreshold ?? 0.0)")
                LogManager.transcription.info("  - Log prob filter: \(settings.logProbThreshold ?? 0.0)")
            }

            // TODO: –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø—Ä–æ–º–ø—Ç–∞ –∫–æ–≥–¥–∞ –ø–æ–ª—É—á–∏–º –¥–æ—Å—Ç—É–ø –∫ tokenizer
            if usePrefill {
                LogManager.transcription.debug("Prefill prompt (\(prefillPrompt.count) chars): \"\(prefillPrompt.prefix(100))...\"")
            }
            if let prompt = promptText, !prompt.isEmpty {
                LogManager.transcription.debug("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç: \"\(prompt.prefix(50))...\"")
            }

            let results = try await whisperKit.transcribe(
                audioArray: processedSamples,
                decodeOptions: options
            )

            // –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            let transcriptionTime = Date().timeIntervalSince(startTime)
            lastTranscriptionTime = transcriptionTime

            // –í—ã—á–∏—Å–ª—è–µ–º Real-Time Factor (RTF)
            // RTF = transcription_time / audio_duration
            // RTF < 1.0 = faster than real-time
            // RTF > 1.0 = slower than real-time
            let rtf = transcriptionTime / audioDuration
            transcriptionCount += 1
            totalRTF += rtf
            averageRTF = totalRTF / Double(transcriptionCount)

            // –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –º–∞—Å—Å–∏–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            guard let firstResult = results.first else {
                LogManager.transcription.failure("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è", message: "–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                return ""
            }

            let transcription = firstResult.text
            let cleanedText = transcription.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines)

            // Apply vocabulary corrections
            let correctedText = vocabularyManager.correctTranscription(cleanedText)

            // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            let speedMultiplier = audioDuration / transcriptionTime
            LogManager.transcription.success(
                "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                details: "\"\(correctedText)\" (\(String(format: "%.2f", transcriptionTime))s, RTF: \(String(format: "%.2f", rtf))x, \(String(format: "%.1f", speedMultiplier))x realtime)"
            )
            LogManager.transcription.debug("Avg RTF: \(String(format: "%.2f", self.averageRTF))x over \(self.transcriptionCount) transcriptions")

            if cleanedText != correctedText {
                LogManager.transcription.debug("Vocabulary correction applied: '\(cleanedText)' -> '\(correctedText)'")
            }

            return correctedText
        } catch {
            LogManager.transcription.failure("–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è", error: error)
            throw WhisperError.transcriptionFailed(error)
        }
    }

    /// –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    public func getPerformanceStats() -> PerformanceStats {
        return PerformanceStats(
            lastTranscriptionTime: lastTranscriptionTime,
            averageRTF: averageRTF,
            transcriptionCount: transcriptionCount,
            modelSize: modelSize
        )
    }

    /// –°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    public func resetPerformanceStats() {
        lastTranscriptionTime = 0
        averageRTF = 0
        transcriptionCount = 0
        totalRTF = 0
        LogManager.transcription.info("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–±—Ä–æ—à–µ–Ω–∞")
    }

    /// –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
    public var isReady: Bool {
        return whisperKit != nil
    }

    deinit {
        LogManager.transcription.info("WhisperService –¥–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    }
}

/// –û—à–∏–±–∫–∏ WhisperService
enum WhisperError: Error {
    case modelNotLoaded
    case modelLoadFailed(Error)
    case transcriptionFailed(Error)
    case invalidAudioFormat

    var localizedDescription: String {
        switch self {
        case .modelNotLoaded:
            return "–ú–æ–¥–µ–ª—å Whisper –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        case .modelLoadFailed(let error):
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: \(error.localizedDescription)"
        case .transcriptionFailed(let error):
            return "–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: \(error.localizedDescription)"
        case .invalidAudioFormat:
            return "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö"
        }
    }
}

/// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
public struct PerformanceStats {
    public let lastTranscriptionTime: TimeInterval
    public let averageRTF: Double
    public let transcriptionCount: Int
    public let modelSize: String

    public var description: String {
        """
        Performance Statistics:
        - Model: \(modelSize)
        - Transcriptions: \(transcriptionCount)
        - Last Time: \(String(format: "%.2f", lastTranscriptionTime))s
        - Average RTF: \(String(format: "%.2f", averageRTF))x
        - Status: \(averageRTF < 1.0 ? "‚úì Faster than realtime" : "‚ö†Ô∏è Slower than realtime")
        """
    }
}
